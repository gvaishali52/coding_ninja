{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent - Combined Cycle Power Plant\n",
    "\n",
    "Combined Cycle Power Plant dataset contains 9568 data points collected from a Combined Cycle Power Plant over 6 years (2006-2011), when the power plant was set to work with full load. \n",
    "Features consist of hourly average ambient variables Temperature (T), Ambient Pressure (AP), Relative Humidity (RH) and Exhaust Vacuum (V) to predict the net hourly electrical energy output (EP) of the plant.\n",
    "\n",
    "You are given:\n",
    "1. A Readme file for more details on dataset. \n",
    "2. A Training dataset csv file with X train and Y train data\n",
    "3. A X test File and you have to predict and submit predictions for this file.\n",
    "\n",
    "Your task is to:\n",
    "1. Code Gradient Descent for N features and come with predictions.\n",
    "2. Try and test with various combinations of learning rates and number of iterations.\n",
    "3. Try using Feature Scaling, and see if it helps you in getting better results. \n",
    "\n",
    "Instructions\n",
    " \n",
    "1. Use Gradient Descent as a training algorithm and submit results predicted.\n",
    "2. Files are in csv format, you can use genfromtxt function in numpy to load data from csv file. Similarly you can use savetxt function to save data into a file.\n",
    "3. Submit a csv file with only predictions for X test data. File should not have any headers and should only have one column i.e. predictions. Also predictions shouldn't be in exponential form.\n",
    "4. Your score is based on coefficient of determination. So it can be possible that nobody gets full score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(points, learning_rate, m , c):\n",
    "    m_slope = 0\n",
    "    c_slope = 0\n",
    "    M = len(points)\n",
    "    for i in range(M):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        m_slope += (-2/M)* (y - m * x - c)*x\n",
    "        c_slope += (-2/M)* (y - m * x - c)\n",
    "    new_m = m - learning_rate * m_slope\n",
    "    new_c = c - learning_rate * c_slope\n",
    "    return new_m, new_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function finds the new cost after each optimisation.\n",
    "def cost(points, m, c):\n",
    "    total_cost = 0\n",
    "    M = len(points)\n",
    "    for i in range(M):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        total_cost += (1/M)*((y - m*x - c)**2)\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(points, learning_rate, num_iterations):\n",
    "    m = 0       # Intial random value taken as 0\n",
    "    c = 0       # Intial random value taken as 0\n",
    "    for i in range(num_iterations):\n",
    "        m, c = step_gradient(points, learning_rate, m , c)\n",
    "        print(i, \" Cost: \", cost(points, m, c))\n",
    "    return m, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data=pd.read_csv('test_ccpp_x_test.csv' ,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('training_ccpp_x_y_train.csv' ,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    data=np.loadtxt('training_ccpp_x_y_train.csv',delimiter=',')\n",
    "    learning_rate=0.0000000000001\n",
    "    num_iterations=131\n",
    "    m,c=gd(data,learning_rate,num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  Cost:  3109.7808856633633\n",
      "1  Cost:  3109.7808851366403\n",
      "2  Cost:  3109.7808846099338\n",
      "3  Cost:  3109.780884083208\n",
      "4  Cost:  3109.780883556488\n",
      "5  Cost:  3109.780883029777\n",
      "6  Cost:  3109.7808825030497\n",
      "7  Cost:  3109.7808819763245\n",
      "8  Cost:  3109.780881449606\n",
      "9  Cost:  3109.780880922901\n",
      "10  Cost:  3109.7808803961757\n",
      "11  Cost:  3109.780879869477\n",
      "12  Cost:  3109.780879342746\n",
      "13  Cost:  3109.780878816011\n",
      "14  Cost:  3109.780878289304\n",
      "15  Cost:  3109.7808777625955\n",
      "16  Cost:  3109.780877235873\n",
      "17  Cost:  3109.780876709155\n",
      "18  Cost:  3109.780876182457\n",
      "19  Cost:  3109.7808756557324\n",
      "20  Cost:  3109.780875129\n",
      "21  Cost:  3109.7808746022783\n",
      "22  Cost:  3109.7808740755404\n",
      "23  Cost:  3109.7808735488593\n",
      "24  Cost:  3109.78087302214\n",
      "25  Cost:  3109.780872495417\n",
      "26  Cost:  3109.780871968698\n",
      "27  Cost:  3109.7808714419775\n",
      "28  Cost:  3109.780870915258\n",
      "29  Cost:  3109.7808703885444\n",
      "30  Cost:  3109.7808698618373\n",
      "31  Cost:  3109.780869335109\n",
      "32  Cost:  3109.7808688083887\n",
      "33  Cost:  3109.7808682816612\n",
      "34  Cost:  3109.780867754949\n",
      "35  Cost:  3109.7808672282404\n",
      "36  Cost:  3109.7808667015215\n",
      "37  Cost:  3109.7808661748136\n",
      "38  Cost:  3109.7808656480856\n",
      "39  Cost:  3109.7808651213772\n",
      "40  Cost:  3109.7808645946366\n",
      "41  Cost:  3109.780864067935\n",
      "42  Cost:  3109.780863541221\n",
      "43  Cost:  3109.7808630144914\n",
      "44  Cost:  3109.780862487776\n",
      "45  Cost:  3109.7808619610523\n",
      "46  Cost:  3109.7808614343335\n",
      "47  Cost:  3109.780860907628\n",
      "48  Cost:  3109.780860380903\n",
      "49  Cost:  3109.780859854169\n",
      "50  Cost:  3109.780859327467\n",
      "51  Cost:  3109.7808588007665\n",
      "52  Cost:  3109.7808582740286\n",
      "53  Cost:  3109.780857747318\n",
      "54  Cost:  3109.7808572205954\n",
      "55  Cost:  3109.7808566938843\n",
      "56  Cost:  3109.7808561671704\n",
      "57  Cost:  3109.7808556404516\n",
      "58  Cost:  3109.7808551137277\n",
      "59  Cost:  3109.7808545870002\n",
      "60  Cost:  3109.780854060275\n",
      "61  Cost:  3109.7808535335816\n",
      "62  Cost:  3109.780853006853\n",
      "63  Cost:  3109.7808524801344\n",
      "64  Cost:  3109.7808519534074\n",
      "65  Cost:  3109.7808514266903\n",
      "66  Cost:  3109.78085089999\n",
      "67  Cost:  3109.7808503732717\n",
      "68  Cost:  3109.780849846567\n",
      "69  Cost:  3109.780849319838\n",
      "70  Cost:  3109.7808487931025\n",
      "71  Cost:  3109.7808482664136\n",
      "72  Cost:  3109.780847739689\n",
      "73  Cost:  3109.7808472129586\n",
      "74  Cost:  3109.7808466862466\n",
      "75  Cost:  3109.7808461595228\n",
      "76  Cost:  3109.7808456328044\n",
      "77  Cost:  3109.7808451060923\n",
      "78  Cost:  3109.780844579349\n",
      "79  Cost:  3109.780844052649\n",
      "80  Cost:  3109.780843525937\n",
      "81  Cost:  3109.7808429992265\n",
      "82  Cost:  3109.7808424724994\n",
      "83  Cost:  3109.780841945775\n",
      "84  Cost:  3109.780841419064\n",
      "85  Cost:  3109.7808408923456\n",
      "86  Cost:  3109.780840365631\n",
      "87  Cost:  3109.780839838902\n",
      "88  Cost:  3109.780839312192\n",
      "89  Cost:  3109.7808387854716\n",
      "90  Cost:  3109.780838258766\n",
      "91  Cost:  3109.780837732034\n",
      "92  Cost:  3109.7808372053105\n",
      "93  Cost:  3109.7808366786116\n",
      "94  Cost:  3109.780836151876\n",
      "95  Cost:  3109.7808356251694\n",
      "96  Cost:  3109.7808350984537\n",
      "97  Cost:  3109.780834571727\n",
      "98  Cost:  3109.7808340450124\n",
      "99  Cost:  3109.7808335182926\n",
      "100  Cost:  3109.78083299158\n",
      "101  Cost:  3109.7808324648568\n",
      "102  Cost:  3109.7808319381393\n",
      "103  Cost:  3109.780831411427\n",
      "104  Cost:  3109.780830884715\n",
      "105  Cost:  3109.7808303579823\n",
      "106  Cost:  3109.780829831274\n",
      "107  Cost:  3109.7808293045423\n",
      "108  Cost:  3109.780828777837\n",
      "109  Cost:  3109.7808282511264\n",
      "110  Cost:  3109.780827724401\n",
      "111  Cost:  3109.780827197677\n",
      "112  Cost:  3109.7808266709644\n",
      "113  Cost:  3109.7808261442406\n",
      "114  Cost:  3109.7808256175267\n",
      "115  Cost:  3109.78082509081\n",
      "116  Cost:  3109.780824564083\n",
      "117  Cost:  3109.7808240373674\n",
      "118  Cost:  3109.7808235106554\n",
      "119  Cost:  3109.780822983951\n",
      "120  Cost:  3109.780822457226\n",
      "121  Cost:  3109.780821930505\n",
      "122  Cost:  3109.7808214037896\n",
      "123  Cost:  3109.7808208770675\n",
      "124  Cost:  3109.780820350358\n",
      "125  Cost:  3109.7808198236357\n",
      "126  Cost:  3109.780819296901\n",
      "127  Cost:  3109.780818770193\n",
      "128  Cost:  3109.7808182434824\n",
      "129  Cost:  3109.780817716751\n",
      "130  Cost:  3109.780817190045\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data.iloc[0:,:-1]\n",
    "y = train_data.iloc[0:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "algo=GradientBoostingRegressor()\n",
    "algo.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sgupt\\anaconda3\\envs\\vaishali\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- 1017.58000000\n",
      "- 11.95000000\n",
      "- 42.03000000\n",
      "- 90.89000000\n",
      "Feature names seen at fit time, yet now missing:\n",
      "-  AP\n",
      "-  RH\n",
      "-  V\n",
      "- # T\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "y_pred=algo.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([472.66297342, 435.27765416, 457.97819249, ..., 438.71921598,\n",
       "       452.8264231 , 445.09956345])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sgupt\\anaconda3\\envs\\vaishali\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- 1017.58000000\n",
      "- 11.95000000\n",
      "- 42.03000000\n",
      "- 90.89000000\n",
      "Feature names seen at fit time, yet now missing:\n",
      "-  AP\n",
      "-  RH\n",
      "-  V\n",
      "- # T\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo.score(test_data,y_pred)\n",
    "# algo.score(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"finalPredictions.csv\",y_pred,delimiter=',', fmt='%.8f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('vaishali')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c626c7f027fd81c21c9d33e27e9da6944415f92c80192dd5d04cdc4b243efcd0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
